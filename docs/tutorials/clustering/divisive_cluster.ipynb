{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set(font_scale=1.75)\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "from graspologic.cluster import DivisiveCluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hierarchical clustering is similar to the clustering algorithms introduced above like AutoGMM and K-Means but it leads to a hierarchy of clusters. Two major types of hierarchical clustering algorithms are agglomerative and divisive. The former one starts from every data point in its own cluster and gradually merges cluters in a \"bottom-up\" fashion; the latter one assumes all data points in the same cluster initially and gradually divides it in a \"top-down\" fashion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This DivisiveCluster algorithm implements hierarchical clustering in a “divisive” approach based on a chosen clustering algorithm such as AutoGMM. It retrieves predictions on the full dataset from the chosen clustering algorithm, say AutoGMM, and passes each subset of data corresponding to a predicted cluster onto AutoGMM again while specifying min_components=1. If the best model computed by AutoGMM for any predicted cluster leads to more than one subcluster, each of the subclusters will be clustered recursively as described above; otherwise, that subcluster becomes a leaf cluster. The algorithm terminates when all branches of recursive clustering have led to a set of leaf clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using DivisiveCluster on Synthetic Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following synthetic hierarchical data made up of two levels of four Gaussian distributions in 2D. The 4 distributions have the same standard deviation. And their means are chosen so that the difference between the first two is the same as that between the last two. Hence, this dataset can be classified into 4 clusters of 1 Gaussian component or 2 clusters of Gaussian mixtures of 4 components. Those are the two clustering hierarchies of increasing granularity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate synethetic data\n",
    "\n",
    "np.random.seed(3)\n",
    "\n",
    "n = 100  # number of data points\n",
    "d = 2  # number of dimensions\n",
    "\n",
    "# Let Xij denote the ith Gaussian mixture component in the jth cluster at the lowest hierarchy, i.e., level 2\n",
    "X11 = np.random.normal(-4.5, 0.5, size=(n, d))\n",
    "X21 = np.random.normal(-3, 0.5, size=(n, d))\n",
    "X12 = np.random.normal(3, 0.5, size=(n, d))\n",
    "X22 = np.random.normal(4.5, 0.5, size=(n, d))\n",
    "X = np.vstack((X11, X21, X12, X22))\n",
    "\n",
    "# true label at either level\n",
    "y_lvl1 = np.repeat([0, 1], 2 * n).reshape((-1, 1))\n",
    "y_lvl2 = np.repeat([0, 1, 2, 3], n).reshape((-1, 1))\n",
    "y = np.hstack((y_lvl1, y_lvl2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting function for clustering\n",
    "def plot(X, y, title):\n",
    "    df = pd.DataFrame(np.hstack((X,y)))\n",
    "    df.columns = [\"dim1\", \"dim2\", \"level1\", \"level2\"]\n",
    "    df[\"level1\"] = df[\"level1\"].astype(int)\n",
    "    df[\"level2\"] = df[\"level2\"].astype(int)\n",
    "    fig,ax = plt.subplots(1, figsize=(10,10))\n",
    "    sns.scatterplot(\n",
    "        data=df, x=df[\"dim1\"], y=df[\"dim2\"], style=df[\"level1\"], hue=df[\"level2\"], palette=\"deep\", ax=ax, legend=False\n",
    "    )\n",
    "    ax.set(xticks=[], yticks=[], title=title)\n",
    "    plt.show()\n",
    "\n",
    "plot(X, y, \"True Clustering\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import adjusted_rand_score\n",
    "np.random.seed(1)\n",
    "\n",
    "# fit model and predict on data\n",
    "dc = DivisiveCluster(max_components=2, cluster_method=\"gmm\", max_level=2)\n",
    "# choose to return a set of flat clusterings\n",
    "pred = dc.fit_predict(X, fcluster=True)\n",
    "\n",
    "# evaluate the clustering performance in terms of ARI \n",
    "# (used before in evaluating other clustering methods) \n",
    "print(\"ARI score for model at level 1: %.2f\" % adjusted_rand_score(y[:,0], pred[:,0]))\n",
    "print(\"ARI score for model at level 2: %.2f\" % adjusted_rand_score(y[:,1], pred[:,1]))\n",
    "\n",
    "plot(X, pred, \"DivisiveCluster Assignments\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
